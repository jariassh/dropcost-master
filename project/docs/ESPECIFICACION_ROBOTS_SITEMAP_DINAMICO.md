# Especificaci√≥n de Requerimientos - Robots.txt y Sitemap Din√°mico
## DropCost Master

**Versi√≥n:** 1.0  
**Fecha:** Febrero 2026  
**Requerimientos:** RF-153 a RF-160  
**Implementador:** Antigravity

---

## 1. Resumen Ejecutivo

Implementar sistema din√°mico para:
- **robots.txt:** Archivo de instrucciones para buscadores (Google, Bing)
- **sitemap.xml:** Mapa din√°mico de todas las p√°ginas indexables

Ambos archivos se generan autom√°ticamente seg√∫n p√°ginas activas en BD.
Se adaptan autom√°ticamente seg√∫n dominio (local ‚Üí testing ‚Üí producci√≥n).

---

## 2. Configuraci√≥n de Dominios

### Dominios por Fase

```
FASE 1 - LOCAL (Desarrollo):
Dominio: http://localhost:3000
Robots.txt: http://localhost:3000/robots.txt
Sitemap: http://localhost:3000/sitemap.xml

FASE 2 - TESTING (Hostinger Temporal):
Dominio: https://silver-gorilla-255825.hostingersite.com
Robots.txt: https://silver-gorilla-255825.hostingersite.com/robots.txt
Sitemap: https://silver-gorilla-255825.hostingersite.com/sitemap.xml

FASE 3 - PRODUCCI√ìN (Dominio Real):
Dominio: https://dropcostmaster.com
Robots.txt: https://dropcostmaster.com/robots.txt
Sitemap: https://dropcostmaster.com/sitemap.xml
```

### Variables de Entorno

```
.env.local (LOCAL):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
VITE_APP_URL=http://localhost:3000
VITE_API_URL=http://localhost:3000/api
NODE_ENV=development

.env.testing (TESTING - Hostinger):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
VITE_APP_URL=https://silver-gorilla-255825.hostingersite.com
VITE_API_URL=https://silver-gorilla-255825.hostingersite.com/api
NODE_ENV=production

.env.production (PRODUCCI√ìN):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
VITE_APP_URL=https://dropcostmaster.com
VITE_API_URL=https://dropcostmaster.com/api
NODE_ENV=production
```

---

## 3. Requerimientos Funcionales

### RF-153: Archivo robots.txt - Estructura y Contenido

**Ubicaci√≥n:** `/public/robots.txt` (ra√≠z del servidor)

**Contenido din√°mico:**

```
User-agent: *
Allow: /
Allow: /registro
Allow: /simulador
Allow: /cursos
Allow: /referidos
Allow: /dashboard
Allow: /ofertas

Disallow: /admin
Disallow: /admin/*
Disallow: /api
Disallow: /api/*
Disallow: /private
Disallow: /private/*
Disallow: /login
Disallow: /*.json$
Disallow: /*.xml$ (excepto sitemap.xml)

User-agent: Googlebot
Allow: /
Crawl-delay: 0

User-agent: Bingbot
Allow: /
Crawl-delay: 1

Sitemap: [DOMINIO]/sitemap.xml
```

**Explicaci√≥n de reglas:**

```
Allow: /                          ‚Üí Permite acceso general
Allow: /registro, /simulador...  ‚Üí P√°ginas espec√≠ficas permitidas
Disallow: /admin                 ‚Üí Bloquea admin (Google no lo indexe)
Disallow: /api                   ‚Üí Bloquea endpoints API
Disallow: /*.json$               ‚Üí Bloquea archivos JSON
Sitemap: [DOMINIO]/sitemap.xml   ‚Üí Apunta a sitemap din√°mico
```

---

### RF-154: Generaci√≥n Din√°mica de robots.txt

**Endpoint:** `GET /robots.txt`

**Implementaci√≥n:**

```typescript
// src/api/routes/robots.ts (o en Supabase Edge Function)

export async function GET_ROBOTS(req: Request) {
  // Obtener dominio de ambiente
  const APP_URL = process.env.VITE_APP_URL || 'http://localhost:3000';
  
  // Construir contenido din√°mico
  const robotsTxt = `User-agent: *
Allow: /
Allow: /registro
Allow: /simulador
Allow: /cursos
Allow: /referidos
Allow: /dashboard
Allow: /ofertas
Allow: /blog

Disallow: /admin
Disallow: /admin/*
Disallow: /api
Disallow: /api/*
Disallow: /private
Disallow: /private/*
Disallow: /login
Disallow: /*.json$
Disallow: *.xml$ (excepto /sitemap.xml)

User-agent: Googlebot
Allow: /
Crawl-delay: 0

User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Sitemap - DIN√ÅMICO SEG√öN DOMINIO
Sitemap: ${APP_URL}/sitemap.xml

# Generado autom√°ticamente el ${new Date().toISOString()}
# Dominio: ${APP_URL}
`;

  return new Response(robotsTxt, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'public, max-age=86400', // Cache 24 horas
    },
  });
}
```

**Ruta en servidor:**

```
Express/Node.js:
app.get('/robots.txt', GET_ROBOTS);

Vercel (Next.js):
pages/robots.txt.ts ‚Üí export default GET_ROBOTS

Supabase Edge Function:
functions/robots/index.ts
```

**Comportamiento:**

```
LOCAL:
GET /robots.txt
‚Üí Sitemap: http://localhost:3000/sitemap.xml

TESTING:
GET /robots.txt
‚Üí Sitemap: https://silver-gorilla-255825.hostingersite.com/sitemap.xml

PRODUCCI√ìN:
GET /robots.txt
‚Üí Sitemap: https://dropcostmaster.com/sitemap.xml

Autom√°tico seg√∫n APP_URL. ‚úÖ
```

---

### RF-155: Archivo sitemap.xml - Estructura

**Ubicaci√≥n:** `/sitemap.xml` (ra√≠z del servidor)

**Estructura XML:**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1"
        xmlns:mobile="http://www.google.com/schemas/sitemap-mobile/1.0">

  <!-- P√°gina principal -->
  <url>
    <loc>[DOMINIO]/</loc>
    <lastmod>2026-02-15T10:30:00Z</lastmod>
    <changefreq>weekly</changefreq>
    <priority>1.0</priority>
  </url>

  <!-- Registro -->
  <url>
    <loc>[DOMINIO]/registro</loc>
    <lastmod>2026-02-15T10:30:00Z</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.9</priority>
  </url>

  <!-- Simulador -->
  <url>
    <loc>[DOMINIO]/simulador</loc>
    <lastmod>2026-02-15T10:30:00Z</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.9</priority>
  </url>

  <!-- Cursos (cada curso es una p√°gina) -->
  <url>
    <loc>[DOMINIO]/cursos</loc>
    <lastmod>2026-02-15T10:30:00Z</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>

  <!-- Referidos -->
  <url>
    <loc>[DOMINIO]/referidos</loc>
    <lastmod>2026-02-15T10:30:00Z</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.7</priority>
  </url>

  <!-- Ofertas -->
  <url>
    <loc>[DOMINIO]/ofertas</loc>
    <lastmod>2026-02-15T10:30:00Z</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>

  <!-- Dashboard (requiere login - OPCIONAL indexar) -->
  <!-- NO incluir /dashboard porque requiere autenticaci√≥n -->

</urlset>
```

---

### RF-156: Generaci√≥n Din√°mica de sitemap.xml

**Endpoint:** `GET /sitemap.xml`

**Implementaci√≥n:**

```typescript
// src/api/routes/sitemap.ts (o Supabase Edge Function)

export async function GET_SITEMAP(req: Request) {
  const APP_URL = process.env.VITE_APP_URL || 'http://localhost:3000';
  const db = createSupabaseClient(); // Conexi√≥n a BD
  
  // 1. Obtener p√°ginas p√∫blicas
  const pagesPrincipales = [
    { path: '/', priority: '1.0', changefreq: 'weekly' },
    { path: '/registro', priority: '0.9', changefreq: 'monthly' },
    { path: '/simulador', priority: '0.9', changefreq: 'weekly' },
    { path: '/cursos', priority: '0.8', changefreq: 'weekly' },
    { path: '/referidos', priority: '0.7', changefreq: 'monthly' },
    { path: '/ofertas', priority: '0.8', changefreq: 'weekly' },
    { path: '/blog', priority: '0.7', changefreq: 'weekly' },
  ];

  // 2. Obtener cursos individuales de BD (si existen)
  const cursos = await db.from('cursos')
    .select('id, slug, fecha_actualizacion')
    .eq('activo', true)
    .eq('publicado', true);

  // 3. Construir XML
  let xml = `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
`;

  // 3.1 Agregar p√°ginas principales
  pagesPrincipales.forEach(page => {
    const lastmod = new Date().toISOString().split('T')[0];
    xml += `  <url>
    <loc>${APP_URL}${page.path}</loc>
    <lastmod>${lastmod}</lastmod>
    <changefreq>${page.changefreq}</changefreq>
    <priority>${page.priority}</priority>
  </url>
`;
  });

  // 3.2 Agregar cursos din√°micamente
  if (cursos.data && cursos.data.length > 0) {
    cursos.data.forEach(curso => {
      const lastmod = new Date(curso.fecha_actualizacion)
        .toISOString()
        .split('T')[0];
      xml += `  <url>
    <loc>${APP_URL}/cursos/${curso.slug}</loc>
    <lastmod>${lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.7</priority>
  </url>
`;
    });
  }

  // 3.3 Cerrar XML
  xml += `</urlset>`;

  return new Response(xml, {
    headers: {
      'Content-Type': 'application/xml; charset=utf-8',
      'Cache-Control': 'public, max-age=3600', // Cache 1 hora
    },
  });
}
```

**Ruta en servidor:**

```
Express/Node.js:
app.get('/sitemap.xml', GET_SITEMAP);

Vercel (Next.js):
pages/sitemap.xml.ts ‚Üí export default GET_SITEMAP

Supabase Edge Function:
functions/sitemap/index.ts
```

**Comportamiento:**

```
Cada vez que alguien accede a /sitemap.xml:
1. Se leen p√°ginas principales
2. Se leen cursos activos de BD
3. Se genera XML din√°mico
4. Se retorna con headers correctos
5. Google lo indexa autom√°ticamente

Sin archivo est√°tico. ‚úÖ
Sin actualizaci√≥n manual. ‚úÖ
```

---

### RF-157: Tabla de P√°ginas Est√°ticas vs Din√°micas

**P√°ginas Est√°ticas (siempre incluir):**

```
/                    (home)
/registro            (sign up)
/simulador           (herramienta principal)
/cursos              (listado cursos)
/referidos           (programa referidos)
/ofertas             (ofertas irresistibles)
/blog                (blog futuro)
/terminos            (t√©rminos y condiciones)
/privacidad          (pol√≠tica privacidad)
/contacto            (formulario contacto)
```

**P√°ginas Din√°micas (de BD):**

```
/cursos/{slug}       (cada curso individual)
/blog/{slug}         (cada post blog)
/ofertas/{id}        (cada oferta)
```

**P√°ginas NO indexar:**

```
/admin/*             (panel admin)
/api/*               (endpoints API)
/login               (login)
/dashboard           (requiere autenticaci√≥n)
/private/*           (p√°ginas privadas)
/*.json              (archivos de datos)
```

---

### RF-158: Generaci√≥n Autom√°tica en Cada Deploy

**Proceso CI/CD:**

```
Cuando Antigravity pushea a producci√≥n:

1. Deploy a servidor (Vercel/Hostinger)
2. Variables de entorno cargadas (VITE_APP_URL)
3. Endpoints /robots.txt y /sitemap.xml activos
4. Google ve robots.txt ‚Üí encuentra sitemap.xml
5. Google indexa todas las p√°ginas

SIN archivo manual. ‚úÖ
SIN pasos manuales. ‚úÖ
```

**Verificaci√≥n:**

```bash
# Verificar robots.txt funciona
curl https://dropcostmaster.com/robots.txt

# Verificar sitemap funciona
curl https://dropcostmaster.com/sitemap.xml

# Verificar Sitemap en robots.txt
grep "Sitemap:" https://dropcostmaster.com/robots.txt
```

---

### RF-159: Configuraci√≥n en Diferentes Dominios

**LOCAL (localhost:3000):**

```
robots.txt:
User-agent: *
Disallow: /
# (Bloquea todo en local, Google no indexa)

Raz√≥n: No queremos que Google indexe localhost
```

**TESTING (silver-gorilla-255825.hostingersite.com):**

```
robots.txt:
User-agent: *
Allow: /
Sitemap: https://silver-gorilla-255825.hostingersite.com/sitemap.xml

Raz√≥n: Queremos que Google vea la estructura
       (pero con dominio temporal, no es problema)
```

**PRODUCCI√ìN (dropcostmaster.com):**

```
robots.txt:
User-agent: *
Allow: /
Sitemap: https://dropcostmaster.com/sitemap.xml

Raz√≥n: Queremos m√°xima indexaci√≥n en dominio real
```

**C√≥digo que lo maneja:**

```typescript
// app.ts
const APP_URL = process.env.VITE_APP_URL;
const isLocal = APP_URL?.includes('localhost');
const isTesting = APP_URL?.includes('hostinger');
const isProduction = APP_URL?.includes('dropcostmaster');

export function GET_ROBOTS(req: Request) {
  let robotsTxt;
  
  if (isLocal) {
    // Bloquear todo en local
    robotsTxt = `User-agent: *\nDisallow: /`;
  } else {
    // Permitir en testing y producci√≥n
    robotsTxt = `User-agent: *\nAllow: /\nSitemap: ${APP_URL}/sitemap.xml`;
  }
  
  return new Response(robotsTxt, { headers: {...} });
}
```

---

### RF-160: Integraci√≥n con Google Search Console

**Despu√©s de deploy en producci√≥n:**

1. Ir a Google Search Console (https://search.google.com/search-console)
2. Agregar propiedad: https://dropcostmaster.com
3. Verificar propiedad (v√≠a DNS o archivo HTML)
4. En secci√≥n "Sitemaps":
   - Click "Agregar sitemap"
   - Ingresar: `https://dropcostmaster.com/sitemap.xml`
   - Google lo procesa autom√°ticamente

**Google luego:**
- Lee /robots.txt
- Encuentra sitemap.xml en robots.txt
- Indexa todas las p√°ginas del sitemap
- Muestra estado de indexaci√≥n en GSC

---

## 4. Checklist de Implementaci√≥n

```
[ ] Variables de entorno configuradas (.env.local, .env.testing, .env.production)
[ ] Endpoint GET /robots.txt implementado
[ ] Endpoint GET /sitemap.xml implementado
[ ] robots.txt redirige correctamente seg√∫n dominio
[ ] sitemap.xml genera din√°micamente desde BD
[ ] Pruebas en localhost: /robots.txt, /sitemap.xml funcionan
[ ] Deploy a Testing: Verificar robots.txt apunta a dominio temporal
[ ] Deploy a Producci√≥n: Verificar robots.txt apunta a dropcostmaster.com
[ ] Agregar sitemap en Google Search Console
[ ] Verificar indexaci√≥n en GSC despu√©s 24-48 horas
[ ] Cache headers configurados (24h robots, 1h sitemap)
```

---

## 5. Testing Local

**Verificar en local:**

```bash
# Terminal 1: Iniciar servidor
npm run dev

# Terminal 2: Probar endpoints
curl http://localhost:3000/robots.txt
curl http://localhost:3000/sitemap.xml

# Debe mostrar contenido correcto con dominio localhost
```

---

## 6. Testing en Hostinger (Fase 2)

**Despu√©s de deploy en Hostinger:**

```bash
# Verificar que redirige a dominio temporal
curl https://silver-gorilla-255825.hostingersite.com/robots.txt

# Debe incluir:
# Sitemap: https://silver-gorilla-255825.hostingersite.com/sitemap.xml
```

---

## 7. Producci√≥n (Fase 3)

**Despu√©s de deploy en dropcostmaster.com:**

```bash
# Verificar que redirige a dominio real
curl https://dropcostmaster.com/robots.txt

# Debe incluir:
# Sitemap: https://dropcostmaster.com/sitemap.xml
```

**Luego en Google Search Console:**
1. Agregar propiedad: https://dropcostmaster.com
2. Agregar sitemap: https://dropcostmaster.com/sitemap.xml
3. Esperar 24-48 horas
4. Ver p√°ginas indexadas en GSC

---

## 8. Notas Importantes

```
1. CACHE:
   - robots.txt: Cache 24 horas (Google lo revisa diariamente)
   - sitemap.xml: Cache 1 hora (cambios son frescos)

2. DOMINIOS:
   - LOCAL: Bloquear indexaci√≥n (no queremos localhost indexado)
   - TESTING: Permitir pero no importa si Google indexa
   - PRODUCCI√ìN: Permitir completamente

3. CAMBIOS:
   - Si cambias VITE_APP_URL ‚Üí reinicia servidor
   - robots.txt y sitemap.xml se regeneran autom√°ticamente
   - No toca c√≥digo, solo cambia variable de entorno

4. GOOGLE INDEXACI√ìN:
   - Google visita /robots.txt primero
   - Ve Sitemap: URL
   - Va a /sitemap.xml
   - Indexa todas las URLs en sitemap

5. VERIFICACI√ìN:
   - site:dropcostmaster.com (b√∫squeda Google)
   - Google Search Console dashboard
   - robots.txt checker (online tools)
```

---

**FIN ESPECIFICACI√ìN RF-153 a RF-160**

---

## üìä RESUMEN

```
RF-153: Estructura robots.txt (din√°mico seg√∫n dominio)
RF-154: Generaci√≥n din√°mica robots.txt
RF-155: Estructura sitemap.xml
RF-156: Generaci√≥n din√°mica sitemap.xml
RF-157: P√°ginas est√°ticas vs din√°micas
RF-158: Automatizaci√≥n en deploy
RF-159: Configuraci√≥n por dominio
RF-160: Integraci√≥n Google Search Console

‚úÖ robots.txt generado din√°micamente
‚úÖ sitemap.xml generado din√°micamente
‚úÖ Se adapta autom√°ticamente al dominio (local ‚Üí testing ‚Üí producci√≥n)
‚úÖ Sin archivos est√°ticos
‚úÖ Sin actualizaciones manuales
‚úÖ Cache configurado
‚úÖ Google indexaci√≥n autom√°tica

IMPLEMENTACI√ìN: 
Antigravity debe crear 2 endpoints:
1. GET /robots.txt
2. GET /sitemap.xml

Ambos leen VITE_APP_URL y generan contenido din√°mico.
```
